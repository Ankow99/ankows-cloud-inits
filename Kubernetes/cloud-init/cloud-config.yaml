#cloud-config
ssh_import_id: ['lp:pgdg99']
ssh_genkeytypes: [ed25519]

write_files:
- path: /etc/netplan/60-lxdbr0.yaml
  permissions: '0600'
  content: |
    network:
      version: 2
      bridges:
        lxdbr0:
          addresses: [10.10.20.1/24]
          parameters:
            stp: false
            forward-delay: 0

- path: /etc/sysctl.d/99-maas-nat.conf
  content: |
    net.ipv4.ip_forward=1

- path: /etc/profile.d/maas-login.sh
  permissions: '0644'
  content: |
    if [ "$USER" = "ubuntu" ] && command -v maas >/dev/null 2>&1; then
        ADMIN_USER="admin"
        maas login $ADMIN_USER 'http://localhost:5240/MAAS/' $(sudo maas apikey --username $ADMIN_USER) >/dev/null 2>&1
    fi

- path: /etc/profile.d/default-editor.sh
  permissions: '0644'
  content: |
    export EDITOR=nvim

- path: /etc/sudoers.d/keep-editor
  permissions: '0440'
  content: |
    Defaults env_keep += "EDITOR"

lxd:
  preseed: |
    config:
      core.https_address: '[::]:8444'
      core.trust_password: password
    storage_pools:
    - config:
        size: 70GB
      description: ""
      name: default
      driver: zfs 
    profiles:
    - config: {}
      description: ""
      devices:
        root:
          path: /
          pool: default
          type: disk
      name: default
    projects: []
    cluster: null

apt:
  sources:
    source1:
      source: ppa:maas/3.6

packages:
  - git
  - neovim
  - pollinate
  - jq
  - iptables-persistent

package_update: true
package_upgrade: true
package_reboot_if_required: true

timezone: America/Bogota

random_seed:
  command: [pollinate]
  command_required: true
  file: /dev/urandom

runcmd:

# Export variables
- echo -e "\n-------- Exporting variables... --------\n"

- export SSH_KEY=lp:pgdg99

- export ADMIN_USER=admin
- export ADMIN_PASS=admin
- export ADMIN_EMAIL=admin@mail.com

- export GATEWAY=10.10.20.1
- export SUBNET=10.10.20.0/24

- export MAAS_PROJECT_NAME=$(hostname)
- export MAAS_DOMAIN_NAME=kubernetes
- export MAAS_DNS=8.8.8.8
- export MAAS_RESERVED_IP_START=10.10.20.200
- export MAAS_RESERVED_IP_END=10.10.20.254

- export NODE_CPU=2
- export NODE_RAM=4096
- export NODE_MAIN_DISK=20

- export IP_ADDRESS=$(ip -j route show default | jq -r '.[].prefsrc')
- export INTERFACE=$(ip -j route show default | jq -r '.[].dev')

# Make the bridge active on the first boot
- echo -e "\n-------- Applying and attaching network bridge... --------\n"

- |
  cat <<EOF > /etc/netplan/99-maas-dns.yaml
  network:
    version: 2
    ethernets:
      $INTERFACE:
        nameservers:
          addresses: [$IP_ADDRESS]
          search: [$MAAS_DOMAIN_NAME, maas]
  EOF

- chmod 600 /etc/netplan/99-maas-dns.yaml

- netplan apply

# Attach lxdbr0 bridge to LXD default profile
- lxc network attach-profile lxdbr0 default

# Fetch IPv4 address from the device, setup forwarding and NAT
- echo -e "\n-------- Configuring Port Forwarding and NAT... --------\n"

- sysctl -p /etc/sysctl.d/99-maas-nat.conf
- iptables -t nat -A POSTROUTING -o $INTERFACE -j SNAT --to $IP_ADDRESS

# Persist NAT configuration
- netfilter-persistent save

# Install MAAS now that LXD and networking are configured
- echo -e "\n-------- Installing MAAS... --------\n"

- apt-get update
- apt-get remove -y systemd-timesyncd
- apt-get install -y maas

# Initialise MAAS
- echo -e "\n-------- Initializing MAAS... --------\n"

- maas init --admin-username $ADMIN_USER --admin-password $ADMIN_PASS --admin-email $ADMIN_EMAIL --admin-ssh-import $SSH_KEY --rbac-url "" --candid-agent-file ""

# Wait for MAAS services to be ready
- |
  echo -e "\n-------- Waiting for MAAS API to be ready... --------\n"
  
  until maas login $ADMIN_USER 'http://localhost:5240/MAAS/' $(maas apikey --username $ADMIN_USER) > /dev/null 2>&1; do
    echo "MAAS is not ready yet (502). Retrying in 30 seconds..."
    sleep 30
  done

# Grab API key and login as admin
- echo -e "\n-------- Logging in as admin user... --------\n"

- export APIKEY=$(maas apikey --username $ADMIN_USER)
- maas login $ADMIN_USER 'http://localhost:5240/MAAS/' $APIKEY

- maas $ADMIN_USER maas set-config name=maas_name value=$MAAS_PROJECT_NAME name=completed_intro value=true

# Automatically create and add ssh keys to MAAS
- echo -e "\n-------- Generating SSH keys and adding them to MAAS... --------\n"

- ssh-keygen -q -t rsa -N "" -f "/home/ubuntu/.ssh/id_rsa"
- chown ubuntu:ubuntu /home/ubuntu/.ssh/id_rsa /home/ubuntu/.ssh/id_rsa.pub
- chmod 600 /home/ubuntu/.ssh/id_rsa
- chmod 644 /home/ubuntu/.ssh/id_rsa.pub
- maas $ADMIN_USER sshkeys create key="$(cat /home/ubuntu/.ssh/id_rsa.pub)"

# Enable root ssh access and distribute public key to root
- cp /home/ubuntu/.ssh/id_rsa* /root/.ssh/

- |
  cat <<EOF > /root/node-deploy-config.yaml
  #cloud-config
  disable_root: false
  users:
    - default
    - name: root
      ssh_authorized_keys:
        - $(cat /root/.ssh/id_rsa.pub)
  runcmd:
    - sed -i 's/^#\?PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config
    - systemctl reload ssh
  EOF

# Configure MAAS networking (set gateways, vlans, DHCP on etc)
- echo -e "\n-------- Configuring MAAS networking... --------\n"

- export FABRIC_ID=$(maas $ADMIN_USER subnet read "$SUBNET" | jq -r ".vlan.fabric_id")
- export VLAN_TAG=$(maas $ADMIN_USER subnet read "$SUBNET" | jq -r ".vlan.vid")
- export PRIMARY_RACK=$(maas $ADMIN_USER rack-controllers read | jq -r ".[] | .system_id")

- maas $ADMIN_USER subnet update $SUBNET gateway_ip=$GATEWAY
- maas $ADMIN_USER ipranges create type=dynamic start_ip=$MAAS_RESERVED_IP_START end_ip=$MAAS_RESERVED_IP_END
- maas $ADMIN_USER vlan update $FABRIC_ID $VLAN_TAG dhcp_on=true primary_rack=$PRIMARY_RACK
- maas $ADMIN_USER maas set-config name=upstream_dns value=$MAAS_DNS

# Create new MAAS domain
- echo -e "\n-------- Creating new k8s domain... --------\n"

- export MAAS_DOMAIN_NAME_ID=$(maas $ADMIN_USER domains create name=$MAAS_DOMAIN_NAME | jq -r '.id')
- maas $ADMIN_USER domain set-default $MAAS_DOMAIN_NAME_ID
- maas $ADMIN_USER rack-controller update $PRIMARY_RACK domain=$MAAS_DOMAIN_NAME

- netplan apply

# Configure boot images
- echo -e "\n-------- Configuring MAAS boot images... --------\n"

- export BOOT_SOURCE_ID=$(maas $ADMIN_USER boot-sources read | jq -r '.[0].id')

- maas $ADMIN_USER boot-source-selections create $BOOT_SOURCE_ID os="ubuntu" release="noble" arches="amd64" labels='*' subarches='*'
- maas $ADMIN_USER boot-source-selections create $BOOT_SOURCE_ID os="ubuntu" release="jammy" arches="amd64" labels='*' subarches='*'

- maas $ADMIN_USER boot-resources import

# Wait for MAAS boot images to be ready
- sleep 90

# Add LXD as a VM host for MAAS
- echo -e "\n-------- Adding LXD as VM host in MAAS... --------\n"

- maas $ADMIN_USER vm-hosts create type=lxd power_address=https://${IP_ADDRESS}:8444 password=password name=LXD project=$MAAS_PROJECT_NAME
- export VM_HOST_ID=$(maas $ADMIN_USER vm-hosts read | jq -r '.[0].id')

# Compose VMs using VM host (-c limits.cpu=6 -c limits.memory=12GiB --device root,size=60GiB)
- echo -e "\n-------- Composing VM(s) using LXD... --------\n"

- export SYSTEM_ID_1=$(maas $ADMIN_USER vm-host compose $VM_HOST_ID cores=$NODE_CPU memory=$NODE_RAM storage=$NODE_MAIN_DISK hostname=server domain=$MAAS_DOMAIN_NAME_ID | jq -r '.system_id')

- export SYSTEM_ID_2=$(maas $ADMIN_USER vm-host compose $VM_HOST_ID cores=$NODE_CPU memory=$NODE_RAM storage=$NODE_MAIN_DISK hostname=node-0 domain=$MAAS_DOMAIN_NAME_ID | jq -r '.system_id')

- export SYSTEM_ID_3=$(maas $ADMIN_USER vm-host compose $VM_HOST_ID cores=$NODE_CPU memory=$NODE_RAM storage=$NODE_MAIN_DISK hostname=node-1 domain=$MAAS_DOMAIN_NAME_ID | jq -r '.system_id')

# Wait for VMs to be ready
- |
  echo "Waiting for VMs to enter 'Ready' state..."
  
  # Aggregate the IDs from your previous exports
  IDS="$SYSTEM_ID_1 $SYSTEM_ID_2 $SYSTEM_ID_3"
  
  # Safety check: Ensure we actually have IDs to check
  if [ -z "$(echo "$IDS" | tr -d ' ')" ]; then
    echo "Error: No System IDs found. Exports may not have persisted."
    exit 1
  fi

  # Start the loop
  while true; do
    all_ready=true
    
    for id in $IDS; do
      # Query MAAS for the status name (requires jq)
      status=$(maas $ADMIN_USER machine read "$id" | jq -r '.status_name')
      echo "  Machine $id is currently '$status'..."
      # If any machine is NOT ready, we mark the flag as false
      if [ "$status" != "Ready" ]; then
         all_ready=false
      fi
    done

    # If the flag is still true, everyone is ready -> break the loop
    if [ "$all_ready" = true ]; then
      echo "All VMs are Ready!"
      break
    fi

    # Wait 30 seconds before checking again
    echo "Waiting 60s before checking again..."
    sleep 60
  done

# Deploy VMs using VM host
- echo -e "\n-------- Deploying VM(s)... --------\n"

- maas $ADMIN_USER machine deploy $SYSTEM_ID_1 distro_series=ubuntu/jammy ephemeral_deploy=false user_data=$(base64 -w 0 /root/node-deploy-config.yaml)
- sleep 60
- maas $ADMIN_USER machine deploy $SYSTEM_ID_2 distro_series=ubuntu/jammy ephemeral_deploy=false user_data=$(base64 -w 0 /root/node-deploy-config.yaml)
- sleep 60
- maas $ADMIN_USER machine deploy $SYSTEM_ID_3 distro_series=ubuntu/jammy ephemeral_deploy=false user_data=$(base64 -w 0 /root/node-deploy-config.yaml)

# Clone Kubernetes the hard way git repo
- echo -e "\n-------- Cloning k8s the hard way repo... --------\n"

- export ARCH=$(dpkg --print-architecture)
- export BASE_DIR="/root/kubernetes-the-hard-way"
- export DOWN_DIR="${BASE_DIR}/downloads"

- git clone --depth 1 https://github.com/kelseyhightower/kubernetes-the-hard-way.git ${BASE_DIR}

# Download files
- echo -e "\n-------- Downloading files... --------\n"

- wget -q --https-only --timestamping -P ${DOWN_DIR} -i ${DOWN_DIR}-${ARCH}.txt && ls -oh ${DOWN_DIR}

# Organize and extract files
- echo -e "\n-------- Extracting and organizing files... --------\n"

# Create directory structure
- mkdir -p ${DOWN_DIR}/client
- mkdir -p ${DOWN_DIR}/cni-plugins
- mkdir -p ${DOWN_DIR}/controller
- mkdir -p ${DOWN_DIR}/worker

# Extract files
- tar -xvf ${DOWN_DIR}/crictl-v1.32.0-linux-${ARCH}.tar.gz -C ${DOWN_DIR}/worker/
- tar -xvf ${DOWN_DIR}/containerd-2.1.0-beta.0-linux-${ARCH}.tar.gz --strip-components 1 -C ${DOWN_DIR}/worker/
- tar -xvf ${DOWN_DIR}/cni-plugins-linux-${ARCH}-v1.6.2.tgz -C ${DOWN_DIR}/cni-plugins/

# Extract etcd specific binaries
- tar -xvf ${DOWN_DIR}/etcd-v3.6.0-rc.3-linux-${ARCH}.tar.gz -C ${DOWN_DIR}/ --strip-components 1 etcd-v3.6.0-rc.3-linux-${ARCH}/etcdctl etcd-v3.6.0-rc.3-linux-${ARCH}/etcd

# Move binaries to respective folders
- mv ${DOWN_DIR}/etcdctl ${DOWN_DIR}/client/
- mv ${DOWN_DIR}/kubectl ${DOWN_DIR}/client/
- mv ${DOWN_DIR}/etcd ${DOWN_DIR}/controller/
- mv ${DOWN_DIR}/kube-apiserver ${DOWN_DIR}/controller/
- mv ${DOWN_DIR}/kube-controller-manager ${DOWN_DIR}/controller/
- mv ${DOWN_DIR}/kube-scheduler ${DOWN_DIR}/controller/
- mv ${DOWN_DIR}/kubelet ${DOWN_DIR}/worker/
- mv ${DOWN_DIR}/kube-proxy ${DOWN_DIR}/worker/
- mv ${DOWN_DIR}/runc.${ARCH} ${DOWN_DIR}/worker/runc

# Cleanup tarballs
- rm -rf ${DOWN_DIR}/*gz && ls -oh ${DOWN_DIR}

# Set as executable
- chmod +x ${DOWN_DIR}/client/*
- chmod +x ${DOWN_DIR}/cni-plugins/*
- chmod +x ${DOWN_DIR}/controller/*
- chmod +x ${DOWN_DIR}/worker/*

# Install kubectl
- echo -e "\n-------- Installing Kubectl... --------\n"

- cp ${DOWN_DIR}/client/kubectl /usr/local/bin/ && kubectl version --client

# Write machine database
- echo -e "\n-------- Writing machine database... --------\n"

- |
  cat <<EOF > /root/machines.txt
  10.10.20.2 server.${MAAS_DOMAIN_NAME} server
  10.10.20.3 node-0.${MAAS_DOMAIN_NAME} node-0 10.200.0.0/24
  10.10.20.4 node-1.${MAAS_DOMAIN_NAME} node-1 10.200.1.0/24
  EOF

- cat /root/machines.txt
- cp /root/machines.txt /home/ubuntu

# Generate client and server certificates
- echo -e "\n-------- Generating client and server certificates... --------\n"

# Replace the .kubernetes.local domains with the .kubernetes domain
- sed -i "/\[kube-api-server_alt_names\]/,/\[.*\]/ s/\.kubernetes\.local/.kubernetes/g" "${BASE_DIR}/ca.conf"

# Replace MAAS Host IP 
- sed -i "/\[kube-api-server_alt_names\]/,/\[.*\]/ s/IP\.1\s*=.*/IP.1  = $IP_ADDRESS/" "${BASE_DIR}/ca.conf"

- |
  openssl genrsa -out ${BASE_DIR}/ca.key 4096
  openssl req -x509 -new -sha512 -noenc -key ${BASE_DIR}/ca.key -days 3653 -config ${BASE_DIR}/ca.conf -out ${BASE_DIR}/ca.crt

  certs="admin node-0 node-1 kube-proxy kube-scheduler kube-controller-manager kube-api-server service-accounts"

  for i in $certs; do
    openssl genrsa -out "${BASE_DIR}/${i}.key" 4096
    openssl req -new -key "${BASE_DIR}/${i}.key" -sha256 -config "${BASE_DIR}/ca.conf" -section ${i} -out "${BASE_DIR}/${i}.csr"
    openssl x509 -req -days 3653 -in "${BASE_DIR}/${i}.csr" -copy_extensions copyall -sha256 -CA "${BASE_DIR}/ca.crt" -CAkey "${BASE_DIR}/ca.key" -CAcreateserial -out "${BASE_DIR}/${i}.crt"
  done

  ls -1 ${BASE_DIR}/*.crt ${BASE_DIR}/*.key ${BASE_DIR}/*.csr

- cp ${BASE_DIR}/ca.crt /home/ubuntu/

# Wait for VMs to be deployed
- echo -e "\n-------- Wait for VMs to be deployed... --------\n"

- |
  echo "Waiting for VMs to enter 'Deployed' state..."
  
  # Aggregate the IDs from your previous exports
  IDS="$SYSTEM_ID_1 $SYSTEM_ID_2 $SYSTEM_ID_3"
  
  # Safety check: Ensure we actually have IDs to check
  if [ -z "$(echo "$IDS" | tr -d ' ')" ]; then
    echo "Error: No System IDs found. Exports may not have persisted."
    exit 1
  fi

  # Start the loop
  while true; do
    all_ready=true
    
    for id in $IDS; do
      # Query MAAS for the status name (requires jq)
      status=$(maas $ADMIN_USER machine read "$id" | jq -r '.status_name')
      echo "  Machine $id is currently '$status'..."
      # If any machine is NOT ready, we mark the flag as false
      if [ "$status" != "Deployed" ]; then
         all_ready=false
      fi
    done

    # If the flag is still true, everyone is ready -> break the loop
    if [ "$all_ready" = true ]; then
      echo "All VMs are Ready!"
      break
    fi

    # Wait 30 seconds before checking again
    echo "Waiting 60s before checking again..."
    sleep 60
  done

- sleep 60

# Distribute client and server certificates
- echo -e "\n-------- Distributing client and server certificates... --------\n"

- ssh-keyscan node-0 node-1 server >> /root/.ssh/known_hosts

- |
  for host in node-0 node-1; do
    ssh -i /root/.ssh/id_rsa root@${host} mkdir /var/lib/kubelet/

    scp -i /root/.ssh/id_rsa ${BASE_DIR}/ca.crt root@${host}:/var/lib/kubelet/

    scp -i /root/.ssh/id_rsa ${BASE_DIR}/${host}.crt root@${host}:/var/lib/kubelet/kubelet.crt

    scp -i /root/.ssh/id_rsa ${BASE_DIR}/${host}.key root@${host}:/var/lib/kubelet/kubelet.key
  done

# Configure the etcd Server
- ssh -i /root/.ssh/id_rsa root@server "mkdir -p /etc/etcd /var/lib/etcd && chmod 700 /var/lib/etcd"

- scp -i /root/.ssh/id_rsa ${BASE_DIR}/ca.crt ${BASE_DIR}/kube-api-server.key ${BASE_DIR}/kube-api-server.crt root@server:/etc/etcd/

# Configure the Kubernetes API Server
- ssh -i /root/.ssh/id_rsa root@server "mkdir -p /var/lib/kubernetes/"

- scp -i /root/.ssh/id_rsa ${BASE_DIR}/ca.crt ${BASE_DIR}/ca.key ${BASE_DIR}/kube-api-server.key ${BASE_DIR}/kube-api-server.crt ${BASE_DIR}/service-accounts.key ${BASE_DIR}/service-accounts.crt root@server:/var/lib/kubernetes/

# Generate client authentication configuration files
- echo -e "\n-------- Generate client authentication configuration files... --------\n"

- |
  # Generate a kubeconfig file for the node-0 and node-1 worker nodes

  for host in node-0 node-1; do
    kubectl config set-cluster kubernetes-the-hard-way \
      --certificate-authority=${BASE_DIR}/ca.crt \
      --embed-certs=true \
      --server=https://server.kubernetes:6443 \
      --kubeconfig=${BASE_DIR}/${host}.kubeconfig

    kubectl config set-credentials system:node:${host} \
      --client-certificate=${BASE_DIR}/${host}.crt \
      --client-key=${BASE_DIR}/${host}.key \
      --embed-certs=true \
      --kubeconfig=${BASE_DIR}/${host}.kubeconfig

    kubectl config set-context default \
      --cluster=kubernetes-the-hard-way \
      --user=system:node:${host} \
      --kubeconfig=${BASE_DIR}/${host}.kubeconfig

    kubectl config use-context default \
      --kubeconfig=${BASE_DIR}/${host}.kubeconfig
  done

  # Generate a kubeconfig file for the kube-proxy service

  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=${BASE_DIR}/ca.crt \
    --embed-certs=true \
    --server=https://server.kubernetes:6443 \
    --kubeconfig=${BASE_DIR}/kube-proxy.kubeconfig

  kubectl config set-credentials system:kube-proxy \
    --client-certificate=${BASE_DIR}/kube-proxy.crt \
    --client-key=${BASE_DIR}/kube-proxy.key \
    --embed-certs=true \
    --kubeconfig=${BASE_DIR}/kube-proxy.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-proxy \
    --kubeconfig=${BASE_DIR}/kube-proxy.kubeconfig

  kubectl config use-context default \
    --kubeconfig=${BASE_DIR}/kube-proxy.kubeconfig

  # Generate a kubeconfig file for the kube-controller-manager service

  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=${BASE_DIR}/ca.crt \
    --embed-certs=true \
    --server=https://server.kubernetes:6443 \
    --kubeconfig=${BASE_DIR}/kube-controller-manager.kubeconfig

  kubectl config set-credentials system:kube-controller-manager \
    --client-certificate=${BASE_DIR}/kube-controller-manager.crt \
    --client-key=${BASE_DIR}/kube-controller-manager.key \
    --embed-certs=true \
    --kubeconfig=${BASE_DIR}/kube-controller-manager.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-controller-manager \
    --kubeconfig=${BASE_DIR}/kube-controller-manager.kubeconfig

  kubectl config use-context default \
    --kubeconfig=${BASE_DIR}/kube-controller-manager.kubeconfig

  # Generate a kubeconfig file for the kube-scheduler service

  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=${BASE_DIR}/ca.crt \
    --embed-certs=true \
    --server=https://server.kubernetes:6443 \
    --kubeconfig=${BASE_DIR}/kube-scheduler.kubeconfig

  kubectl config set-credentials system:kube-scheduler \
    --client-certificate=${BASE_DIR}/kube-scheduler.crt \
    --client-key=${BASE_DIR}/kube-scheduler.key \
    --embed-certs=true \
    --kubeconfig=${BASE_DIR}/kube-scheduler.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-scheduler \
    --kubeconfig=${BASE_DIR}/kube-scheduler.kubeconfig

  kubectl config use-context default \
    --kubeconfig=${BASE_DIR}/kube-scheduler.kubeconfig

  # Generate a kubeconfig file for the admin user:

  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=${BASE_DIR}/ca.crt \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=${BASE_DIR}/admin.kubeconfig

  kubectl config set-credentials admin \
    --client-certificate=${BASE_DIR}/admin.crt \
    --client-key=${BASE_DIR}/admin.key \
    --embed-certs=true \
    --kubeconfig=${BASE_DIR}/admin.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=admin \
    --kubeconfig=${BASE_DIR}/admin.kubeconfig

  kubectl config use-context default \
    --kubeconfig=${BASE_DIR}/admin.kubeconfig

# Distribute client authentication configuration files
- echo -e "\n-------- Distribute client authentication configuration files... --------\n"

- |
  for host in node-0 node-1; do
    ssh -i /root/.ssh/id_rsa root@${host} "mkdir -p /var/lib/{kube-proxy,kubelet}"

    scp -i /root/.ssh/id_rsa ${BASE_DIR}/kube-proxy.kubeconfig root@${host}:/var/lib/kube-proxy/kubeconfig

    scp -i /root/.ssh/id_rsa ${BASE_DIR}/${host}.kubeconfig root@${host}:/var/lib/kubelet/kubeconfig
  done

- scp -i /root/.ssh/id_rsa ${BASE_DIR}/kube-controller-manager.kubeconfig ${BASE_DIR}/kube-scheduler.kubeconfig root@server:/var/lib/kubernetes/

- scp -i /root/.ssh/id_rsa ${BASE_DIR}/admin.kubeconfig root@server:~/

# Generate the data encryption config and key
- echo -e "\n-------- Generate the data encryption config and key... --------\n"

- export ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)

- envsubst < ${BASE_DIR}/configs/encryption-config.yaml > ${BASE_DIR}/encryption-config.yaml

- scp -i /root/.ssh/id_rsa ${BASE_DIR}/encryption-config.yaml root@server:/var/lib/kubernetes/

# Copy etcd binaries and systemd unit files to the server machine
- echo -e "\n-------- Copying etcd binaries and systemd unit files to the server machine... --------\n"

# Install the etcd server and the etcdctl command line utility
- scp -i /root/.ssh/id_rsa ${DOWN_DIR}/controller/etcd ${DOWN_DIR}/client/etcdctl root@server:/usr/local/bin/

# Create the etcd.service systemd unit file
- scp -i /root/.ssh/id_rsa ${BASE_DIR}/units/etcd.service root@server:/etc/systemd/system/

# Start etcd server
- echo -e "\n-------- Starting etcd server.. --------\n"

- ssh -i /root/.ssh/id_rsa root@server "systemctl daemon-reload && systemctl enable etcd && systemctl start etcd && etcdctl member list"

# Copy Kubernetes binaries and systemd unit files to the server machine
- echo -e "\n-------- Copying Kubernetes binaries and systemd unit files to the server machine... --------\n"

- ssh -i /root/.ssh/id_rsa root@server "mkdir -p /etc/kubernetes/config"

# Install the Kubernetes binaries
- scp -i /root/.ssh/id_rsa ${DOWN_DIR}/controller/kube-apiserver ${DOWN_DIR}/controller/kube-controller-manager ${DOWN_DIR}/controller/kube-scheduler ${DOWN_DIR}/client/kubectl root@server:/usr/local/bin/

# Create the kube-apiserver.service, kube-controller-manager.service and kube-scheduler.service systemd unit files
- scp -i /root/.ssh/id_rsa ${BASE_DIR}/units/kube-apiserver.service ${BASE_DIR}/units/kube-controller-manager.service ${BASE_DIR}/units/kube-scheduler.service root@server:/etc/systemd/system/

- scp -i /root/.ssh/id_rsa ${BASE_DIR}/configs/kube-scheduler.yaml root@server:/etc/kubernetes/config/

# Start controller services
- echo -e "\n-------- Start controller services.. --------\n"

- ssh -i /root/.ssh/id_rsa root@server "systemctl daemon-reload && systemctl enable kube-apiserver kube-controller-manager kube-scheduler && systemctl start kube-apiserver kube-controller-manager kube-scheduler"

- sleep 30

# Check if the kube-apiserver is fully initialized, and active
- ssh -i /root/.ssh/id_rsa root@server "systemctl is-active kube-apiserver && kubectl cluster-info --kubeconfig /root/admin.kubeconfig"

- scp -i /root/.ssh/id_rsa ${BASE_DIR}/configs/kube-apiserver-to-kubelet.yaml root@server:~/

- ssh -i /root/.ssh/id_rsa root@server "kubectl apply -f /root/kube-apiserver-to-kubelet.yaml --kubeconfig /root/admin.kubeconfig"

# Verify Kubernetes control plane
- echo -e "\n-------- Verify Kubernetes control plane.. --------\n"

- curl --cacert ${BASE_DIR}/ca.crt https://server.kubernetes:6443/version

# Install the OS dependencies
- echo -e "\n-------- Installing dependencies in each worker instance... --------\n"

- |
  for HOST in node-0 node-1; do
    ssh -i /root/.ssh/id_rsa root@${HOST} "apt-get update && apt-get -y install socat conntrack ipset kmod"
    ssh -i /root/.ssh/id_rsa root@${HOST} "swapoff -a"
    ssh -i /root/.ssh/id_rsa root@${HOST} "mkdir -p /etc/cni/net.d /opt/cni/bin /var/lib/kubelet /var/lib/kube-proxy /var/lib/kubernetes /var/run/kubernetes /etc/containerd/"
  done

# Copy Kubernetes binaries and systemd unit files to each worker instance
- echo -e "\n-------- Copying Kubernetes binaries and systemd unit files to each worker instance... --------\n"

- |
  for HOST in node-0 node-1; do
    SUBNET=$(grep ${HOST} /root/machines.txt | cut -d " " -f 4)
    sed "s|SUBNET|$SUBNET|g" ${BASE_DIR}/configs/10-bridge.conf > ${BASE_DIR}/10-bridge.conf

    sed "s|SUBNET|$SUBNET|g" ${BASE_DIR}/configs/kubelet-config.yaml > ${BASE_DIR}/kubelet-config.yaml

    scp -i /root/.ssh/id_rsa ${BASE_DIR}/10-bridge.conf ${BASE_DIR}/configs/99-loopback.conf root@${HOST}:/etc/cni/net.d/

    scp -i /root/.ssh/id_rsa ${BASE_DIR}/kubelet-config.yaml root@${HOST}:/var/lib/kubelet/
  done

  for HOST in node-0 node-1; do
    scp -i /root/.ssh/id_rsa ${DOWN_DIR}/worker/crictl ${DOWN_DIR}/worker/kube-proxy ${DOWN_DIR}/worker/kubelet ${DOWN_DIR}/worker/runc root@${HOST}:/usr/local/bin/
    scp -i /root/.ssh/id_rsa ${DOWN_DIR}/worker/containerd ${DOWN_DIR}/worker/containerd-shim-runc-v2 ${DOWN_DIR}/worker/containerd-stress root@${HOST}:/bin/
    
    scp -i /root/.ssh/id_rsa ${BASE_DIR}/configs/containerd-config.toml root@${HOST}:/etc/containerd/
    scp -i /root/.ssh/id_rsa ${BASE_DIR}/configs/kube-proxy-config.yaml root@${HOST}:/var/lib/kube-proxy/

    scp -i /root/.ssh/id_rsa ${BASE_DIR}/units/containerd.service ${BASE_DIR}/units/kubelet.service ${BASE_DIR}/units/kube-proxy.service root@${HOST}:/etc/systemd/system/
  done

# Configure CNI Networking in each worker instance
- echo -e "\n-------- Configure CNI Networking in each worker instance... --------\n"

- |
  for HOST in node-0 node-1; do
    scp -i /root/.ssh/id_rsa ${DOWN_DIR}/cni-plugins/* root@${HOST}:/opt/cni/bin
    ssh -i /root/.ssh/id_rsa root@${HOST} "modprobe br-netfilter && echo "br-netfilter" >> /etc/modules-load.d/modules.conf"
    ssh -i /root/.ssh/id_rsa root@${HOST} "echo "net.bridge.bridge-nf-call-iptables = 1" >> /etc/sysctl.d/kubernetes.conf"
    ssh -i /root/.ssh/id_rsa root@${HOST} "echo "net.bridge.bridge-nf-call-ip6tables = 1" >> /etc/sysctl.d/kubernetes.conf"
    ssh -i /root/.ssh/id_rsa root@${HOST} "sysctl -p /etc/sysctl.d/kubernetes.conf"
  done

# Start worker services in each worker instance
- echo -e "\n-------- Start worker services in each worker instance... --------\n"

- |
  for HOST in node-0 node-1; do
    ssh -i /root/.ssh/id_rsa root@${HOST} "systemctl daemon-reload && systemctl enable containerd kubelet kube-proxy && systemctl start containerd kubelet kube-proxy"
  done

- sleep 30

# Verify if the Kubelet service is running in each worker instance
- |
  for HOST in node-0 node-1; do
    echo -e "\n-------- Verify if the Kubelet service is running in '${HOST}'.. --------\n"
    ssh -i /root/.ssh/id_rsa root@${HOST} "systemctl is-active kubelet"
  done

# Verify registered Kubernetes nodes
- echo -e "\n-------- Verify registered Kubernetes nodes.. --------\n"

- ssh root@server "kubectl get nodes --kubeconfig ~/admin.kubeconfig"

# Configure kubectl for remote access
- echo -e "\n-------- Configuring kubectl for remote access... --------\n"

- |
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=${BASE_DIR}/ca.crt \
    --embed-certs=true \
    --server=https://server.kubernetes:6443 \
    --kubeconfig=/root/.kube/config

  kubectl config set-credentials admin \
    --client-certificate=${BASE_DIR}/admin.crt \
    --client-key=${BASE_DIR}/admin.key \
    --embed-certs=true \
    --kubeconfig=/root/.kube/config

  kubectl config set-context kubernetes-the-hard-way \
    --cluster=kubernetes-the-hard-way \
    --user=admin \
    --kubeconfig=/root/.kube/config

  kubectl config use-context kubernetes-the-hard-way \
    --kubeconfig=/root/.kube/config

- cp -r /root/.kube /home/ubuntu/
- chown -R ubuntu:ubuntu /home/ubuntu/.kube

# Verify Kubernetes cluster
- echo -e "\n-------- Verify Kubernetes cluster... --------\n"

- su -l root -c "kubectl version && kubectl get nodes"

# Create routing table
- echo -e "\n-------- Create routing table... --------\n"

- export SERVER_IP=$(grep server /root/machines.txt | cut -d " " -f 1)
- export NODE_0_IP=$(grep node-0 /root/machines.txt | cut -d " " -f 1)
- export NODE_0_SUBNET=$(grep node-0 /root/machines.txt | cut -d " " -f 4)
- export NODE_1_IP=$(grep node-1 /root/machines.txt | cut -d " " -f 1)
- export NODE_1_SUBNET=$(grep node-1 /root/machines.txt | cut -d " " -f 4)

- ssh -i /root/.ssh/id_rsa root@server "ip route add ${NODE_0_SUBNET} via ${NODE_0_IP}"
- ssh -i /root/.ssh/id_rsa root@server "ip route add ${NODE_1_SUBNET} via ${NODE_1_IP}"

- ssh -i /root/.ssh/id_rsa root@node-0 "ip route add ${NODE_1_SUBNET} via ${NODE_1_IP}"
- ssh -i /root/.ssh/id_rsa root@node-1 "ip route add ${NODE_0_SUBNET} via ${NODE_0_IP}"

# Verify routing table
- echo -e "\n-------- Verify routing table... --------\n"

- ssh root@server ip route
- ssh root@node-0 ip route
- ssh root@node-1 ip route

# Kubernetes the hard way finished deploying
- echo -e "\n-------- Kubernetes the hard way finished deploying! --------\n"
